2023-09-10 00:10:10,446 [INFO] chromadb.telemetry.posthog: Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-09-10 00:10:10,468 [INFO] clickhouse_connect.driver.ctypes: Successfully imported ClickHouse Connect C data optimizations
2023-09-10 00:10:10,477 [INFO] clickhouse_connect.json_impl: Using python library for writing JSON byte strings
2023-09-10 00:10:10,717 [INFO] chromadb.db.duckdb: loaded in 6027 embeddings
2023-09-10 00:10:10,718 [INFO] chromadb.db.duckdb: loaded in 1 collections
2023-09-10 00:10:10,719 [INFO] chromadb.db.duckdb: collection with name langchain already exists, returning existing collection
2023-09-10 00:10:10,722 [INFO] chromadb.telemetry.posthog: Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-09-10 00:10:10,786 [INFO] chromadb.db.duckdb: loaded in 2010 embeddings
2023-09-10 00:10:10,787 [INFO] chromadb.db.duckdb: loaded in 1 collections
2023-09-10 00:10:10,787 [INFO] chromadb.db.duckdb: collection with name langchain already exists, returning existing collection
2023-09-10 00:10:14,453 [INFO] convo_qa_chain: user_input: What are the hyper-parameters of Position-wise Feed-Forward Networks?
2023-09-10 00:10:14,454 [INFO] convo_qa_chain: new_question_list: ['What are the hyper-parameters of Position-wise Feed-Forward Networks?']
2023-09-10 00:10:15,470 [INFO] toolkit.retrivers: ----1st retrieval----: page_content='inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 4, 'small_chunk_idx': 71, 'large_chunks_idx_lower_bound': 18, 'large_chunks_idx_upper_bound': 23, 'page_content': 'inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks', 'page_content_md5': 'de9921c82967d7074d58d2e6aed4a8e0', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:10:15,470 [INFO] toolkit.retrivers: ----1st retrieval----: page_content='sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 2, 'small_chunk_idx': 39, 'large_chunks_idx_lower_bound': 8, 'large_chunks_idx_upper_bound': 13, 'page_content': 'sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of', 'page_content_md5': '5436c13f30ff4e2fb80b24532dafa216', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:10:15,470 [INFO] toolkit.retrivers: ----1st retrieval----: page_content='[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 10, 'small_chunk_idx': 194, 'large_chunks_idx_lower_bound': 59, 'large_chunks_idx_upper_bound': 64, 'page_content': '[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.', 'page_content_md5': '393649756438e046cdc9d7fb962e7e65', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:10:15,470 [INFO] toolkit.retrivers: ----1st retrieval----: page_content='In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 4, 'small_chunk_idx': 72, 'large_chunks_idx_lower_bound': 19, 'large_chunks_idx_upper_bound': 24, 'page_content': 'In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This', 'page_content_md5': '599c22ce298a27bb5cd4a8acbb46789a', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:10:15,910 [INFO] toolkit.retrivers: ids_clean: [3]
2023-09-10 00:10:15,910 [INFO] toolkit.retrivers: selected_docs_at_1st_retrieval: ./data/Attention Is All You Need.pdf
2023-09-10 00:10:16,148 [INFO] toolkit.retrivers: ----2nd retrieval----: page_content='inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 4, 'small_chunk_idx': 71, 'large_chunks_idx_lower_bound': 18, 'large_chunks_idx_upper_bound': 23, 'page_content': 'inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks', 'page_content_md5': 'de9921c82967d7074d58d2e6aed4a8e0', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:10:16,148 [INFO] toolkit.retrivers: ----2nd retrieval----: page_content='[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 10, 'small_chunk_idx': 194, 'large_chunks_idx_lower_bound': 59, 'large_chunks_idx_upper_bound': 64, 'page_content': '[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.', 'page_content_md5': '393649756438e046cdc9d7fb962e7e65', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:10:16,148 [INFO] toolkit.retrivers: ----2nd retrieval----: page_content='In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This' metadata={'page_content_md5': '599c22ce298a27bb5cd4a8acbb46789a', 'source': './data/Attention Is All You Need.pdf', 'page': 4, 'small_chunk_idx': 72, 'large_chunks_idx_upper_bound': 24, 'source_md5': 'c85c8b852377f722a146823b79e1e3a5', 'page_content': 'In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This', 'large_chunks_idx_lower_bound': 19}
2023-09-10 00:10:16,148 [INFO] toolkit.retrivers: windows_at_2nd_retrieval: [21, 61]
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. Thisconsists of two linear transformations with a ReLU activation in between.\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)While the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.']
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 4, 'large_chunks_idx_lower_bound': 19, 'large_chunks_idx_upper_bound': 24, 'small_chunk_idx_lower_bound': 72, 'small_chunk_idx_upper_bound': 74, 'medium_chunk_idx': 24, 'page_content': None, 'page_content_md5': '2a20a741672342d13dc7b342805d4d90', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['encoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend toall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement thisinside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks']
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 4, 'large_chunks_idx_lower_bound': 18, 'large_chunks_idx_upper_bound': 23, 'small_chunk_idx_lower_bound': 69, 'small_chunk_idx_upper_bound': 71, 'medium_chunk_idx': 23, 'page_content': None, 'page_content_md5': 'a04cc09e96d17d10c8999ca8f747cc6a', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357 , 2016.[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluationof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.']
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 10, 'large_chunks_idx_lower_bound': 57, 'large_chunks_idx_upper_bound': 62, 'small_chunk_idx_lower_bound': 186, 'small_chunk_idx_upper_bound': 188, 'medium_chunk_idx': 62, 'page_content': None, 'page_content_md5': '1e31083cfcd0b5d66876f260fa80d5bb', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL , 2016.[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint']
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'medium_chunk_idx': 63, 'source': './data/Attention Is All You Need.pdf', 'small_chunk_idx_upper_bound': 191, 'large_chunks_idx_upper_bound': 63, 'page': 10, 'small_chunk_idx_lower_bound': 189, 'source_md5': 'c85c8b852377f722a146823b79e1e3a5', 'page_content_md5': '456b5c5fc252ada20204048aac0e5e13', 'page_content': None, 'large_chunks_idx_lower_bound': 58}
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['arXiv:1308.0850 , 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 770–778, 2016.[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.']
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 10, 'large_chunks_idx_lower_bound': 59, 'large_chunks_idx_upper_bound': 64, 'small_chunk_idx_lower_bound': 192, 'small_chunk_idx_upper_bound': 194, 'medium_chunk_idx': 64, 'page_content': None, 'page_content_md5': 'fd619f685e8cfa8c419fab0e0c4ae963', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\n9(8):1735–1780, 1997.[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in NaturalLanguage Processing , pages 832–841. ACL, August 2009.']
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'page_content_md5': 'e0011c6b66cc83f77969baad3298f5f8', 'large_chunks_idx_lower_bound': 60, 'source': './data/Attention Is All You Need.pdf', 'small_chunk_idx_upper_bound': 197, 'medium_chunk_idx': 65, 'page': 10, 'source_md5': 'c85c8b852377f722a146823b79e1e3a5', 'large_chunks_idx_upper_bound': 65, 'small_chunk_idx_lower_bound': 195, 'page_content': None}
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['i, KWK\ni, V WV\ni)\nWhere the projections are parameter matrices WQ\ni∈Rdmodel×dk,WK\ni∈Rdmodel×dk,WVi∈Rdmodel×dv\nandWO∈Rhdv×dmodel.\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we usedk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model']
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 4, 'large_chunks_idx_lower_bound': 16, 'large_chunks_idx_upper_bound': 21, 'small_chunk_idx_lower_bound': 63, 'small_chunk_idx_upper_bound': 65, 'medium_chunk_idx': 21, 'page_content': None, 'page_content_md5': 'bcd4aa8090aa71b248231f050afb1371', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['10[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR , abs/1406.1078, 2014.']
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'large_chunks_idx_upper_bound': 61, 'page_lower_bound': 9, 'small_chunk_idx_upper_bound': 185, 'source': './data/Attention Is All You Need.pdf', 'page_content_md5': '7b04fb95a8e02d4a82898ca5ce9bc995', 'page_content': None, 'large_chunks_idx_lower_bound': 56, 'small_chunk_idx_lower_bound': 183, 'source_md5': 'c85c8b852377f722a146823b79e1e3a5', 'medium_chunk_idx': 61, 'page_upper_bound': 10}
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and SoftmaxSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-mation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax']
2023-09-10 00:10:16,601 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 4, 'large_chunks_idx_lower_bound': 20, 'large_chunks_idx_upper_bound': 25, 'small_chunk_idx_lower_bound': 75, 'small_chunk_idx_upper_bound': 77, 'medium_chunk_idx': 25, 'page_content': None, 'page_content_md5': '81a7c71d19e528e86d7a57b2fce56928', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:10:16,604 [INFO] convo_qa_chain: -----step_done--------------------------------------------------
2023-09-10 00:10:19,260 [INFO] convo_qa_chain: *****response*****: The hyper-parameters of Position-wise Feed-Forward Networks in the Attention Is All You Need model are as follows:

- The dimensionality of the input and output: dmodel = 512
- The dimensionality of the inner-layer: dff = 2048

These hyper-parameters determine the size and complexity of the feed-forward network used in each position of the encoder and decoder layers.
2023-09-10 00:10:19,260 [INFO] convo_qa_chain: =====epoch_done============================================================
2023-09-10 00:32:34,366 [INFO] chromadb.db.duckdb: Persisting DB to disk, putting it in the save folder: database_store/chroma_openAIEmbeddings_chunks_medium
2023-09-10 00:32:35,058 [INFO] chromadb.db.duckdb: Persisting DB to disk, putting it in the save folder: database_store/chroma_openAIEmbeddings_chunks_small
2023-09-10 00:32:40,564 [INFO] chromadb.telemetry.posthog: Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-09-10 00:32:40,586 [INFO] clickhouse_connect.driver.ctypes: Successfully imported ClickHouse Connect C data optimizations
2023-09-10 00:32:40,595 [INFO] clickhouse_connect.json_impl: Using python library for writing JSON byte strings
2023-09-10 00:32:40,835 [INFO] chromadb.db.duckdb: loaded in 6027 embeddings
2023-09-10 00:32:40,836 [INFO] chromadb.db.duckdb: loaded in 1 collections
2023-09-10 00:32:40,837 [INFO] chromadb.db.duckdb: collection with name langchain already exists, returning existing collection
2023-09-10 00:32:40,839 [INFO] chromadb.telemetry.posthog: Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-09-10 00:32:40,903 [INFO] chromadb.db.duckdb: loaded in 2010 embeddings
2023-09-10 00:32:40,903 [INFO] chromadb.db.duckdb: loaded in 1 collections
2023-09-10 00:32:40,904 [INFO] chromadb.db.duckdb: collection with name langchain already exists, returning existing collection
2023-09-10 00:32:43,792 [INFO] convo_qa_chain: user_input: How does Position-wise Feed-Forward Networks work in Transformer?
2023-09-10 00:32:43,792 [INFO] convo_qa_chain: new_question_list: ['How does Position-wise Feed-Forward Networks work in Transformer?']
2023-09-10 00:32:44,763 [INFO] toolkit.retrivers: ----1st retrieval----: page_content='inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 4, 'small_chunk_idx': 71, 'large_chunks_idx_lower_bound': 18, 'large_chunks_idx_upper_bound': 23, 'page_content': 'inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks', 'page_content_md5': 'de9921c82967d7074d58d2e6aed4a8e0', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:32:44,763 [INFO] toolkit.retrivers: ----1st retrieval----: page_content='In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 4, 'small_chunk_idx': 72, 'large_chunks_idx_lower_bound': 19, 'large_chunks_idx_upper_bound': 24, 'page_content': 'In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This', 'page_content_md5': '599c22ce298a27bb5cd4a8acbb46789a', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:32:44,763 [INFO] toolkit.retrivers: ----1st retrieval----: page_content='Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 2, 'small_chunk_idx': 37, 'large_chunks_idx_lower_bound': 7, 'large_chunks_idx_upper_bound': 12, 'page_content': 'Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', 'page_content_md5': '64852b90e1870ebc2f9a3b86121e9b5c', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:32:44,763 [INFO] toolkit.retrivers: ----1st retrieval----: page_content='In this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 9, 'small_chunk_idx': 172, 'large_chunks_idx_lower_bound': 52, 'large_chunks_idx_upper_bound': 57, 'page_content': 'In this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.', 'page_content_md5': 'caf62b48f69aec95c13815e603049d13', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:32:44,763 [INFO] toolkit.retrivers: ----1st retrieval----: page_content='are used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 1, 'small_chunk_idx': 23, 'large_chunks_idx_lower_bound': 2, 'large_chunks_idx_upper_bound': 7, 'page_content': 'are used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.', 'page_content_md5': '19e9b9478315e5a041995316d6515846', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:32:45,106 [INFO] toolkit.retrivers: ids_clean: [2]
2023-09-10 00:32:45,107 [INFO] toolkit.retrivers: selected_docs_at_1st_retrieval: ./data/Attention Is All You Need.pdf
2023-09-10 00:32:45,431 [INFO] toolkit.retrivers: ----2nd retrieval----: page_content='inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 4, 'small_chunk_idx': 71, 'large_chunks_idx_lower_bound': 18, 'large_chunks_idx_upper_bound': 23, 'page_content': 'inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks', 'page_content_md5': 'de9921c82967d7074d58d2e6aed4a8e0', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:32:45,431 [INFO] toolkit.retrivers: ----2nd retrieval----: page_content='Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 2, 'small_chunk_idx': 37, 'large_chunks_idx_lower_bound': 7, 'large_chunks_idx_upper_bound': 12, 'page_content': 'Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', 'page_content_md5': '64852b90e1870ebc2f9a3b86121e9b5c', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:32:45,431 [INFO] toolkit.retrivers: ----2nd retrieval----: page_content='are used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.' metadata={'small_chunk_idx': 23, 'page': 1, 'large_chunks_idx_lower_bound': 2, 'page_content': 'are used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.', 'large_chunks_idx_upper_bound': 7, 'source': './data/Attention Is All You Need.pdf', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5', 'page_content_md5': '19e9b9478315e5a041995316d6515846'}
2023-09-10 00:32:45,431 [INFO] toolkit.retrivers: windows_at_2nd_retrieval: [9, 20]
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['encoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend toall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement thisinside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks']
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 4, 'large_chunks_idx_lower_bound': 18, 'large_chunks_idx_upper_bound': 23, 'small_chunk_idx_lower_bound': 69, 'small_chunk_idx_upper_bound': 71, 'medium_chunk_idx': 23, 'page_content': None, 'page_content_md5': 'a04cc09e96d17d10c8999ca8f747cc6a', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. Thisconsists of two linear transformations with a ReLU activation in between.\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)While the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.']
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 4, 'large_chunks_idx_lower_bound': 19, 'large_chunks_idx_upper_bound': 24, 'small_chunk_idx_lower_bound': 72, 'small_chunk_idx_upper_bound': 74, 'medium_chunk_idx': 24, 'page_content': None, 'page_content_md5': '2a20a741672342d13dc7b342805d4d90', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each ofthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layeritself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512 .']
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 2, 'large_chunks_idx_lower_bound': 8, 'large_chunks_idx_upper_bound': 13, 'small_chunk_idx_lower_bound': 39, 'small_chunk_idx_upper_bound': 41, 'medium_chunk_idx': 13, 'page_content': None, 'page_content_md5': '0be57166596dcc85b572f736df624144', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['i, KWK\ni, V WV\ni)\nWhere the projections are parameter matrices WQ\ni∈Rdmodel×dk,WK\ni∈Rdmodel×dk,WVi∈Rdmodel×dv\nandWO∈Rhdv×dmodel.\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we usedk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model']
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'small_chunk_idx_upper_bound': 65, 'large_chunks_idx_lower_bound': 16, 'page': 4, 'medium_chunk_idx': 21, 'large_chunks_idx_upper_bound': 21, 'page_content': None, 'page_content_md5': 'bcd4aa8090aa71b248231f050afb1371', 'small_chunk_idx_lower_bound': 63, 'source': './data/Attention Is All You Need.pdf', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['The Transformer uses multi-head attention in three different ways:\n•In "encoder-decoder attention" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows everyposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the']
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 4, 'large_chunks_idx_lower_bound': 17, 'large_chunks_idx_upper_bound': 22, 'small_chunk_idx_lower_bound': 66, 'small_chunk_idx_upper_bound': 68, 'medium_chunk_idx': 22, 'page_content': None, 'page_content_md5': '0ae3a760070a7e2b64d88571edb6ba8f', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequenceof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive']
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 1, 'large_chunks_idx_lower_bound': 6, 'large_chunks_idx_upper_bound': 11, 'small_chunk_idx_lower_bound': 33, 'small_chunk_idx_upper_bound': 35, 'medium_chunk_idx': 11, 'page_content': None, 'page_content_md5': 'ea8c01086a6bab7adb1d5d4466d63fde', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been']
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 1, 'large_chunks_idx_lower_bound': 4, 'large_chunks_idx_upper_bound': 9, 'small_chunk_idx_lower_bound': 27, 'small_chunk_idx_upper_bound': 29, 'medium_chunk_idx': 9, 'page_content': None, 'page_content_md5': 'abf856e2d2af82c51a0fdb2a044bec3e', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['used successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-']
2023-09-10 00:32:45,645 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'small_chunk_idx_upper_bound': 32, 'page_content': None, 'page': 1, 'large_chunks_idx_lower_bound': 5, 'source': './data/Attention Is All You Need.pdf', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5', 'page_content_md5': '270b1639fddc2cdb444620117d351224', 'large_chunks_idx_upper_bound': 10, 'medium_chunk_idx': 10, 'small_chunk_idx_lower_bound': 30}
2023-09-10 00:32:45,646 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['[10], consuming the previously generated symbols as additional input when generating the next.\n2Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two']
2023-09-10 00:32:45,646 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'large_chunks_idx_lower_bound': 7, 'page_content': None, 'large_chunks_idx_upper_bound': 12, 'medium_chunk_idx': 12, 'small_chunk_idx_lower_bound': 36, 'small_chunk_idx_upper_bound': 38, 'page_lower_bound': 1, 'source': './data/Attention Is All You Need.pdf', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5', 'page_upper_bound': 2, 'page_content_md5': 'cb9fefd3f47832f1e69e9d8bf02c4c71'}
2023-09-10 00:32:45,649 [INFO] convo_qa_chain: -----step_done--------------------------------------------------
2023-09-10 00:32:53,369 [INFO] convo_qa_chain: *****response*****: In the Transformer model, the Position-wise Feed-Forward Networks (FFN) are used in both the encoder and decoder layers. The FFN is a fully connected feed-forward network that operates on each position separately and identically within a layer.

The FFN consists of two linear transformations with a ReLU activation function in between. Mathematically, it can be represented as:

FFN(x) = max(0, xW1 + b1)W2 + b2

Here, x represents the input at each position, W1 and W2 are weight matrices, b1 and b2 are bias vectors, and max(0, x) denotes the ReLU activation function.

The FFN applies the same linear transformations to each position in parallel, allowing the model to capture different features at different positions. This helps in modeling complex relationships and capturing non-linear patterns within the input sequence.

It's important to note that the parameters of the FFN are different from layer to layer, allowing the model to learn different representations at different depths. This can be seen as applying two convolutions with a kernel size of 1.

Overall, the Position-wise Feed-Forward Networks in the Transformer model contribute to the model's ability to capture and process information at each position in the sequence independently, enhancing its representation and modeling capabilities.
2023-09-10 00:32:53,369 [INFO] convo_qa_chain: =====epoch_done============================================================
2023-09-10 00:37:15,010 [INFO] chromadb.db.duckdb: Persisting DB to disk, putting it in the save folder: database_store/chroma_openAIEmbeddings_chunks_medium
2023-09-10 00:37:15,334 [INFO] chromadb.db.duckdb: Persisting DB to disk, putting it in the save folder: database_store/chroma_openAIEmbeddings_chunks_small
2023-09-10 00:37:20,575 [INFO] chromadb.telemetry.posthog: Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-09-10 00:37:20,595 [INFO] clickhouse_connect.driver.ctypes: Successfully imported ClickHouse Connect C data optimizations
2023-09-10 00:37:20,602 [INFO] clickhouse_connect.json_impl: Using python library for writing JSON byte strings
2023-09-10 00:37:20,881 [INFO] chromadb.db.duckdb: loaded in 6027 embeddings
2023-09-10 00:37:20,882 [INFO] chromadb.db.duckdb: loaded in 1 collections
2023-09-10 00:37:20,883 [INFO] chromadb.db.duckdb: collection with name langchain already exists, returning existing collection
2023-09-10 00:37:20,885 [INFO] chromadb.telemetry.posthog: Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.
2023-09-10 00:37:20,963 [INFO] chromadb.db.duckdb: loaded in 2010 embeddings
2023-09-10 00:37:20,963 [INFO] chromadb.db.duckdb: loaded in 1 collections
2023-09-10 00:37:20,964 [INFO] chromadb.db.duckdb: collection with name langchain already exists, returning existing collection
2023-09-10 00:37:23,877 [INFO] convo_qa_chain: user_input: What do hyper-parameters in Position-wise Feed-Forward Networks stand for？
2023-09-10 00:37:23,878 [INFO] convo_qa_chain: new_question_list: ['What do hyper-parameters in Position-wise Feed-Forward Networks stand for？']
2023-09-10 00:37:24,848 [INFO] toolkit.retrivers: ----1st retrieval----: page_content='inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 4, 'small_chunk_idx': 71, 'large_chunks_idx_lower_bound': 18, 'large_chunks_idx_upper_bound': 23, 'page_content': 'inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks', 'page_content_md5': 'de9921c82967d7074d58d2e6aed4a8e0', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:37:24,848 [INFO] toolkit.retrivers: ----1st retrieval----: page_content='In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 4, 'small_chunk_idx': 72, 'large_chunks_idx_lower_bound': 19, 'large_chunks_idx_upper_bound': 24, 'page_content': 'In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This', 'page_content_md5': '599c22ce298a27bb5cd4a8acbb46789a', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:37:24,848 [INFO] toolkit.retrivers: ----1st retrieval----: page_content='the initial human-chosen hyperparameters. To validate this behavior of higher-order hyperoptimizers,\nwe ran each of our benchmarks from above (MLP on MNIST, CNN on CIFAR-10, RNN on Tolstoy)' metadata={'source': './data/Gradient Descent The Ultimate Optimizer.pdf', 'page': 7, 'small_chunk_idx': 178, 'large_chunks_idx_lower_bound': 54, 'large_chunks_idx_upper_bound': 59, 'page_content': 'the initial human-chosen hyperparameters. To validate this behavior of higher-order hyperoptimizers,\nwe ran each of our benchmarks from above (MLP on MNIST, CNN on CIFAR-10, RNN on Tolstoy)', 'page_content_md5': '079d2726ed8a68b52f5a57daded7256c', 'source_md5': 'ccc7bacd0b9c5fcda18620dbd721a01e'}
2023-09-10 00:37:24,848 [INFO] toolkit.retrivers: ----1st retrieval----: page_content='CNNs, and RNNs) our hyperoptimizers are robust to suboptimal choices of initial hyperparameters,\nand that this robustness increases as we grow the stacks of optimizers taller.\n2 Implementing hyperoptimizers' metadata={'source': './data/Gradient Descent The Ultimate Optimizer.pdf', 'page': 1, 'small_chunk_idx': 23, 'large_chunks_idx_lower_bound': 2, 'large_chunks_idx_upper_bound': 7, 'page_content': 'CNNs, and RNNs) our hyperoptimizers are robust to suboptimal choices of initial hyperparameters,\nand that this robustness increases as we grow the stacks of optimizers taller.\n2 Implementing hyperoptimizers', 'page_content_md5': '156f408fa992d6e633e44791b43b5dc7', 'source_md5': 'ccc7bacd0b9c5fcda18620dbd721a01e'}
2023-09-10 00:37:24,848 [INFO] toolkit.retrivers: ----1st retrieval----: page_content='sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 2, 'small_chunk_idx': 39, 'large_chunks_idx_lower_bound': 8, 'large_chunks_idx_upper_bound': 13, 'page_content': 'sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of', 'page_content_md5': '5436c13f30ff4e2fb80b24532dafa216', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:37:25,190 [INFO] toolkit.retrivers: ids_clean: [1]
2023-09-10 00:37:25,190 [INFO] toolkit.retrivers: selected_docs_at_1st_retrieval: ./data/Attention Is All You Need.pdf
2023-09-10 00:37:25,427 [INFO] toolkit.retrivers: ----2nd retrieval----: page_content='inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 4, 'small_chunk_idx': 71, 'large_chunks_idx_lower_bound': 18, 'large_chunks_idx_upper_bound': 23, 'page_content': 'inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks', 'page_content_md5': 'de9921c82967d7074d58d2e6aed4a8e0', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:37:25,427 [INFO] toolkit.retrivers: ----2nd retrieval----: page_content='sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of' metadata={'source': './data/Attention Is All You Need.pdf', 'page': 2, 'small_chunk_idx': 39, 'large_chunks_idx_lower_bound': 8, 'large_chunks_idx_upper_bound': 13, 'page_content': 'sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of', 'page_content_md5': '5436c13f30ff4e2fb80b24532dafa216', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:37:25,427 [INFO] toolkit.retrivers: ----2nd retrieval----: page_content='In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This' metadata={'large_chunks_idx_lower_bound': 19, 'large_chunks_idx_upper_bound': 24, 'small_chunk_idx': 72, 'source_md5': 'c85c8b852377f722a146823b79e1e3a5', 'page_content': 'In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This', 'page': 4, 'page_content_md5': '599c22ce298a27bb5cd4a8acbb46789a', 'source': './data/Attention Is All You Need.pdf'}
2023-09-10 00:37:25,427 [INFO] toolkit.retrivers: windows_at_2nd_retrieval: [21, 10]
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. Thisconsists of two linear transformations with a ReLU activation in between.\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)While the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.']
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 4, 'large_chunks_idx_lower_bound': 19, 'large_chunks_idx_upper_bound': 24, 'small_chunk_idx_lower_bound': 72, 'small_chunk_idx_upper_bound': 74, 'medium_chunk_idx': 24, 'page_content': None, 'page_content_md5': '2a20a741672342d13dc7b342805d4d90', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['encoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend toall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement thisinside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks']
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 4, 'large_chunks_idx_lower_bound': 18, 'large_chunks_idx_upper_bound': 23, 'small_chunk_idx_lower_bound': 69, 'small_chunk_idx_upper_bound': 71, 'medium_chunk_idx': 23, 'page_content': None, 'page_content_md5': 'a04cc09e96d17d10c8999ca8f747cc6a', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each ofthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layeritself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512 .']
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 2, 'large_chunks_idx_lower_bound': 8, 'large_chunks_idx_upper_bound': 13, 'small_chunk_idx_lower_bound': 39, 'small_chunk_idx_upper_bound': 41, 'medium_chunk_idx': 13, 'page_content': None, 'page_content_md5': '0be57166596dcc85b572f736df624144', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['predictions for position ican depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.']
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'large_chunks_idx_lower_bound': 10, 'large_chunks_idx_upper_bound': 15, 'page_lower_bound': 2, 'page_upper_bound': 3, 'small_chunk_idx_lower_bound': 45, 'small_chunk_idx_upper_bound': 47, 'medium_chunk_idx': 15, 'page_content': None, 'page_content_md5': 'ab525e6f4d2f9c9dcd29d848c760eaee', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['used successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-']
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'page_content_md5': '270b1639fddc2cdb444620117d351224', 'page': 1, 'large_chunks_idx_lower_bound': 5, 'large_chunks_idx_upper_bound': 10, 'small_chunk_idx_lower_bound': 30, 'source_md5': 'c85c8b852377f722a146823b79e1e3a5', 'small_chunk_idx_upper_bound': 32, 'page_content': None, 'source': './data/Attention Is All You Need.pdf', 'medium_chunk_idx': 10}
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['i, KWK\ni, V WV\ni)\nWhere the projections are parameter matrices WQ\ni∈Rdmodel×dk,WK\ni∈Rdmodel×dk,WVi∈Rdmodel×dv\nandWO∈Rhdv×dmodel.\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we usedk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model']
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 4, 'large_chunks_idx_lower_bound': 16, 'large_chunks_idx_upper_bound': 21, 'small_chunk_idx_lower_bound': 63, 'small_chunk_idx_upper_bound': 65, 'medium_chunk_idx': 21, 'page_content': None, 'page_content_md5': 'bcd4aa8090aa71b248231f050afb1371', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['linear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\n5Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernelsize of convolutions and rthe size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2·d) O(1) O(1)']
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'page_lower_bound': 4, 'medium_chunk_idx': 26, 'large_chunks_idx_lower_bound': 21, 'large_chunks_idx_upper_bound': 26, 'page_content': None, 'page_content_md5': '02d4909c7e0cc9f3bb00d21b8cc40d9d', 'small_chunk_idx_lower_bound': 78, 'source_md5': 'c85c8b852377f722a146823b79e1e3a5', 'small_chunk_idx_upper_bound': 80, 'source': './data/Attention Is All You Need.pdf', 'page_upper_bound': 5}
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----page_content: ['The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and SoftmaxSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-mation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax']
2023-09-10 00:37:25,650 [INFO] toolkit.retrivers: ----3rd retrieval----metadata: {'source': './data/Attention Is All You Need.pdf', 'page': 4, 'large_chunks_idx_lower_bound': 20, 'large_chunks_idx_upper_bound': 25, 'small_chunk_idx_lower_bound': 75, 'small_chunk_idx_upper_bound': 77, 'medium_chunk_idx': 25, 'page_content': None, 'page_content_md5': '81a7c71d19e528e86d7a57b2fce56928', 'source_md5': 'c85c8b852377f722a146823b79e1e3a5'}
2023-09-10 00:37:25,654 [INFO] convo_qa_chain: -----step_done--------------------------------------------------
2023-09-10 00:37:29,339 [INFO] convo_qa_chain: *****response*****: In the Position-wise Feed-Forward Networks, there are two hyperparameters:

1. dmodel: It represents the dimensionality of the input and output vectors in the feed-forward network. In the given context, dmodel is set to 512.

2. dff: It represents the dimensionality of the inner-layer in the feed-forward network. In the given context, dff is set to 2048.

These hyperparameters determine the size and complexity of the feed-forward network and can be adjusted to optimize the performance of the model.
2023-09-10 00:37:29,340 [INFO] convo_qa_chain: =====epoch_done============================================================
2023-09-10 00:40:02,874 [INFO] chromadb.db.duckdb: Persisting DB to disk, putting it in the save folder: database_store/chroma_openAIEmbeddings_chunks_medium
2023-09-10 00:40:03,064 [INFO] chromadb.db.duckdb: Persisting DB to disk, putting it in the save folder: database_store/chroma_openAIEmbeddings_chunks_small
